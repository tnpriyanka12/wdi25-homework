<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title></title>
        <link rel="stylesheet" href="css/style.css">
        <style>
        @import url('https://fonts.googleapis.com/css?family=Spectral+SC');
        @import url('https://fonts.googleapis.com/css?family=Barlow');
        @import url('https://fonts.googleapis.com/css?family=Orbitron');
        </style>
  </head>
  <body>
    <header>
      <nav>
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="BLD.html">Birth, Life, Death etc.,</a></li>
          <li><a href="https://en.wikipedia.org/wiki/Claude_Shannon">Wiki</a></li>
        </ul>
          </nav>
    </header>
    <div class="main">
      <h2>Claude Shannon - Information Theory</h2>
      <br>
      <marquee direction="left" behavior="alternate" scrollamount="12">
        <img src="images/csMouse2.jpg" width="250" height="250"></marquee>

      <iframe width="560" height="315" src="https://www.youtube.com/embed/vPKkXibQXGA" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

      <article>
        <br>
        <p>In 1948, the promised memorandum appeared as "A Mathematical Theory of Communication," an article in two parts in the July and October issues of the Bell System Technical Journal. This work focuses on the problem of how best to encode the information a sender wants to transmit. In this fundamental work, he used tools in probability theory, developed by Norbert Wiener, which were in their nascent stages of being applied to communication theory at that time. Shannon developed information entropy as a measure of the uncertainty in a message while essentially inventing the field of information theory.
        </p><br>
        <p>The book, co-authored with Warren Weaver, The Mathematical Theory of Communication, reprints Shannon's 1948 article and Weaver's popularization of it, which is accessible to the non-specialist. Warren Weaver pointed out that the word information in communication theory is not related to what you do say, but to what you could say. That is, information is a measure of one's freedom of choice when one selects a message. Shannon's concepts were also popularized, subject to his own proofreading, in John Robinson Pierce's Symbols, Signals, and Noise.
        </p><br>
        <p>Information theory's fundamental contribution to natural language processing and computational linguistics was further established in 1951, in his article "Prediction and Entropy of Printed English", showing upper and lower bounds of entropy on the statistics of English â€“ giving a statistical foundation to language analysis. In addition, he proved that treating whitespace as the 27th letter of the alphabet actually lowers uncertainty in written language, providing a clear quantifiable link between cultural practice and probabilistic cognition.
        </p><br>
        <p>Another notable paper published in 1949 is "Communication Theory of Secrecy Systems", a declassified version of his wartime work on the mathematical theory of cryptography, in which he proved that all theoretically unbreakable ciphers must have the same requirements as the one-time pad. He is also credited with the introduction of sampling theory, which is concerned with representing a continuous-time signal from a (uniform) discrete set of samples. This theory was essential in enabling telecommunications to move from analog to digital transmissions systems in the 1960s and later.He returned to MIT to hold an endowed chair in 1956.</p><br>


      </article>

    </div>
    <footer>
    <p>&copy; Grant Hanrahan 2017</p>
    </footer>
  </body>
</html>
